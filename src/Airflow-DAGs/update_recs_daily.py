from datetime import datetime, timedelta
from pathlib import Path
import logging
import pickle

import pandas as pd
import pyodbc
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.utils.dates import days_ago

# Náº¿u báº¡n Ä‘Ã£ táº¡o Connection trong Airflow (Admin â†’ Connections) vá»›i Conn ID = "libol_sql",
# báº¡n cÃ³ thá»ƒ dÃ¹ng Airflow Hook thay vÃ¬ hardcode chuá»—i sau.
SQL_CONN_PARAMS = {
    "DRIVER": "{ODBC Driver 17 for SQL Server}",
    "SERVER": "192.168.150.6",
    "DATABASE": "libol",
    "UID": "itc",
    "PWD": "spkt@2025",
    "timeout": 30
}

SQL_QUERY_NEW = """
SELECT b.ID AS user_id, l.Tai_lieu_ID AS item_id
FROM Lich_su_muon_sach l
JOIN Ban_doc b ON l.So_the_ID = b.ID
WHERE l.Ngay_muon >= ?  -- yesterday 00:00
  AND l.Ngay_muon <  ?; -- today 00:00
"""

PICKLE_PATH = Path("/home/lib/rs_flask/model/userBorrowDict.pkl")

DEFAULT_ARGS = {
    "owner": "airflow",
    "depends_on_past": False,
    "retries": 1,
    "retry_delay": timedelta(minutes=5),
}


def extract_new_borrows():
    logger = logging.getLogger("airflow.task")

    # 1) XÃ¡c Ä‘á»‹nh khoáº£ng thá»i gian: tá»« hÃ´m qua 00:00 Ä‘áº¿n hÃ´m nay 00:00
    today     = datetime.now().date()
    yesterday = today - timedelta(days=1)

    # 2) Äá»c dá»¯ liá»‡u má»›i tá»« SQL Server
    conn = pyodbc.connect(**SQL_CONN_PARAMS)
    df   = pd.read_sql(SQL_QUERY_NEW, conn, params=[yesterday, today])
    conn.close()

    # 3) Load history (pickle) â€” chuyá»ƒn sang dÃ¹ng set cho má»—i user Ä‘á»ƒ check nhanh
    if PICKLE_PATH.exists():
        with PICKLE_PATH.open("rb") as f:
            user_history: dict[int, set[int]] = pickle.load(f)
    else:
        user_history = {}

    # 4) TÃ¬m nhá»¯ng borrow má»›i
    new_records: list[tuple[int,int]] = []
    for _, row in df.iterrows():
        uid  = int(row["user_id"])
        item = int(row["item_id"])
        seen = user_history.setdefault(uid, set())
        if item not in seen:
            seen.add(item)
            new_records.append((uid, item))

    # 5) Chá»‰ ghi file láº¡i náº¿u cÃ³ borrow má»›i
    if new_records:
        PICKLE_PATH.parent.mkdir(parents=True, exist_ok=True)
        with PICKLE_PATH.open("wb") as f:
            pickle.dump(user_history, f)

        logger.info(f"âœ… Added {len(new_records)} new borrows:")
        for uid, item in new_records:
            logger.info(f"   - user {uid} borrowed item {item}")
    else:
        logger.info("ðŸ”„ No new borrows found.")

with DAG(
    dag_id="daily_update_borrow_and_retrain",
    default_args=DEFAULT_ARGS,
    schedule_interval="0 0 * * *",    # cháº¡y 00:00 UTC má»—i ngÃ y
    start_date=days_ago(1),
    catchup=False,
    tags=["recommender", "daily"],
) as dag:

    extract_task = PythonOperator(
        task_id="extract_new_borrows",
        python_callable=extract_new_borrows,
    )

    extract_task
